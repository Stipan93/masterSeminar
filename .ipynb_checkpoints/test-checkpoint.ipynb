{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import SnowballStemmer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "class FeatureSet:\n",
    "    def __init__(self, data, train_len, validation_len, test_len):\n",
    "        self.data = np.array(data)\n",
    "        self.train_len = train_len\n",
    "        self.validation_len = validation_len\n",
    "        self.test_len = test_len\n",
    "\n",
    "\n",
    "class Data:\n",
    "    def __init__(self, corpus):\n",
    "        self.corpus = corpus\n",
    "        \n",
    "    def print_stats(self):\n",
    "        print(\"Corpus: \", self.corpus)\n",
    "        print(\"\\ntrain:\")\n",
    "        self.train.print_stats()\n",
    "        print(\"\\nvalidation:\")\n",
    "        self.validation.print_stats()\n",
    "        print(\"\\ntest:\")\n",
    "        self.test.print_stats()\n",
    "\n",
    "    def get_set(self, set):\n",
    "        if set == 'train':\n",
    "            return self.train\n",
    "        elif set == 'validation':\n",
    "            return self.validation\n",
    "        elif set == 'test':\n",
    "            return self.test\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self):\n",
    "        self.documents = []\n",
    "        self.entity_counter = {}\n",
    "\n",
    "    def add_document(self, document):\n",
    "        self.documents.append(document)\n",
    "\n",
    "    def print_stats(self):\n",
    "        print(\"Entity counters:\")\n",
    "        for key in self.entity_counter:\n",
    "            print(key, \" -> \", self.entity_counter[key])\n",
    "\n",
    "    def last_doc(self):\n",
    "        return self.documents[len(self.documents) - 1]\n",
    "\n",
    "    def merge(self, dataset):\n",
    "        self.documents.extend(dataset.documents)\n",
    "        self.entity_counter.update(dataset.entity_counter)\n",
    "\n",
    "\n",
    "class Document:\n",
    "    def __init__(self):\n",
    "        self.sentences = []\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        self.sentences.append(sentence)\n",
    "\n",
    "    def remove_last_sentence(self):\n",
    "        self.sentences.pop()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \" \".join(self.sentences)\n",
    "\n",
    "\n",
    "class Sentence:\n",
    "    def __init__(self):\n",
    "        self.words = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        self.words.append(word)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \" \".join(self.words)\n",
    "\n",
    "\n",
    "class Word:\n",
    "    def __init__(self, token, pos, entity, stem):\n",
    "        self.token = token\n",
    "        self.pos = pos\n",
    "        self.entity = entity\n",
    "        self.stem = stem\n",
    "        # self.tag = tag\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.token\n",
    "\n",
    "\n",
    "def get_data(corpus):\n",
    "    return Data(corpus)\n",
    "\n",
    "\n",
    "def read_datasets(path):\n",
    "    filename = path.rsplit('/', 1)[-1]\n",
    "    if filename.startswith('eng'):\n",
    "        return read_eng_dataset(path)\n",
    "    elif filename.startswith('esp'):\n",
    "        return read_esp_dataset(path)\n",
    "    elif filename.startswith('ned'):\n",
    "        return read_ned_dataset(path)\n",
    "\n",
    "\n",
    "def read_eng_dataset(path):\n",
    "    dataset = Dataset()\n",
    "    document = sentence = None\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    with open(path, encoding='utf-8', mode='r') as f:\n",
    "        new_sentence = True\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            if line == '':\n",
    "                break\n",
    "            if line.startswith('-DOCSTART-'):\n",
    "                document = Document()\n",
    "                dataset.add_document(document)\n",
    "            elif line.strip() == '':\n",
    "                new_sentence = True\n",
    "            else:\n",
    "                if new_sentence:\n",
    "                    sentence = Sentence()\n",
    "                    document.add_sentence(sentence)\n",
    "                    new_sentence = False\n",
    "                args = line.split()\n",
    "                ent_type = entity = None\n",
    "                sentence.add_word(Word(args[0], args[1], args[3], stemmer.stem(args[0])))\n",
    "                if args[3] != 'O':\n",
    "                    ent_type, entity = args[3].split('-', 2)\n",
    "                if ent_type == 'B':\n",
    "                    dataset.entity_counter[entity] = dataset.entity_counter.get(entity, 0) + 1\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def read_esp_dataset(path):\n",
    "    dataset = Dataset()\n",
    "    document = Document()\n",
    "    sentence = None\n",
    "    stemmer = SnowballStemmer(\"spanish\")\n",
    "    dataset.add_document(document)\n",
    "    with open(path, encoding='latin-1', mode='r') as f:\n",
    "        new_sentence = True\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            if line == '':\n",
    "                break\n",
    "            if line.strip() == '':\n",
    "                new_sentence = True\n",
    "            else:\n",
    "                if new_sentence:\n",
    "                    sentence = Sentence()\n",
    "                    document.add_sentence(sentence)\n",
    "                    new_sentence = False\n",
    "                args = line.split()\n",
    "                ent_type = entity = None\n",
    "                sentence.add_word(Word(args[0], None, args[1], stemmer.stem(args[0])))\n",
    "                if args[1] != 'O':\n",
    "                    ent_type, entity = args[1].split('-', 2)\n",
    "                if ent_type == 'B':\n",
    "                    dataset.entity_counter[entity] = dataset.entity_counter.get(entity, 0) + 1\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def read_ned_dataset(path):\n",
    "    dataset = Dataset()\n",
    "    document = sentence = None\n",
    "    stemmer = SnowballStemmer(\"dutch\")\n",
    "    with open(path, encoding='latin-1', mode='r') as f:\n",
    "        new_sentence = True\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            if line == '':\n",
    "                break\n",
    "            if line.startswith('-DOCSTART-'):\n",
    "                document = Document()\n",
    "                dataset.add_document(document)\n",
    "            elif line.strip() == '':\n",
    "                new_sentence = True\n",
    "            else:\n",
    "                if new_sentence:\n",
    "                    sentence = Sentence()\n",
    "                    document.add_sentence(sentence)\n",
    "                    new_sentence = False\n",
    "                args = line.split()\n",
    "                ent_type = entity = None\n",
    "                sentence.add_word(Word(args[0], args[1], args[2], stemmer.stem(args[0])))\n",
    "                if args[2] != 'O':\n",
    "                    ent_type, entity = args[2].split('-', 2)\n",
    "                if ent_type == 'B':\n",
    "                    dataset.entity_counter[entity] = dataset.entity_counter.get(entity, 0) + 1\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_starting(entity):\n",
    "    return entity.replace('I-', 'B-')\n",
    "\n",
    "\n",
    "def normalize_flags(flags):\n",
    "    new_flags = []\n",
    "    prev_flag = -2\n",
    "    for flag in flags:\n",
    "        if flag != prev_flag + 1:\n",
    "            new_flags.append(flag)\n",
    "        prev_flag = flag\n",
    "    return new_flags\n",
    "\n",
    "\n",
    "def next_entity(index, words, entity, f, counter):\n",
    "    while index < len(words) and words[index].entity == entity:\n",
    "        f.write(words[index].token + \" \" + words[index].pos + \" - \" + entity + \"\\n\")\n",
    "        index += 1\n",
    "        counter += 1\n",
    "    return index - 1, counter\n",
    "\n",
    "\n",
    "def transfer_eng_to_bio(dataset, output):\n",
    "    counter = 0\n",
    "    flags = []\n",
    "    eng = read_datasets(project_path.get_dataset('eng_old_encoding', dataset))\n",
    "    with open(output, 'w') as f:\n",
    "        for doc in eng.documents:\n",
    "            f.write(\"-DOCSTART- -X- -X- O\\n\\n\")\n",
    "            counter += 2\n",
    "            for sentence in doc.sentences:\n",
    "                index = 0\n",
    "                sentence_length = len(sentence.words)\n",
    "                words = sentence.words\n",
    "                while index < sentence_length:\n",
    "                    entity = words[index].entity\n",
    "                    if entity.startswith('B-'):\n",
    "                        flags.append(counter)\n",
    "                    if entity == 'O':\n",
    "                        f.write(words[index].token + \" \" + words[index].pos + \" - \" + entity + \"\\n\")\n",
    "                    else:\n",
    "                        if entity.startswith('B'):\n",
    "                            f.write(words[index].token + \" \" + words[index].pos + \" - \" + entity + \"\\n\")\n",
    "                            index, counter = next_entity(index + 1, words, entity.replace(\"B-\", \"I-\"), f, counter)\n",
    "                        else:\n",
    "                            f.write(words[index].token + \" \" + words[index].pos + \" - \" + get_starting(entity) + \"\\n\")\n",
    "                            index, counter = next_entity(index + 1, words, entity, f, counter)\n",
    "                    index += 1\n",
    "                    counter += 1\n",
    "\n",
    "                f.write(\"\\n\")\n",
    "                counter += 1\n",
    "    return flags\n",
    "\n",
    "\n",
    "def count_entity_size_dataset(dataset, counter):\n",
    "    for document in dataset.documents:\n",
    "        for sentence in document.sentences:\n",
    "            i = size = 0\n",
    "            n = len(sentence.words)\n",
    "            entity = None\n",
    "            while i < n:\n",
    "                word = sentence.words[i]\n",
    "                if word.entity == 'O':\n",
    "                    if entity is not None:\n",
    "                        entity_size_list = counter.get(entity, [])\n",
    "                        entity_size_list.append(size)\n",
    "                        counter[entity] = entity_size_list\n",
    "                    entity = None\n",
    "                    size = 0\n",
    "                else:\n",
    "                    if word.entity.startswith('I-'):\n",
    "                        size += 1\n",
    "                    elif word.entity.startswith('B-'):\n",
    "                        if entity is not None:\n",
    "                            entity_size_list = counter.get(entity, [])\n",
    "                            entity_size_list.append(size)\n",
    "                            counter[entity] = entity_size_list\n",
    "                        entity = word.entity.split('-', 2)[-1]\n",
    "                        size = 1\n",
    "                i += 1\n",
    "            if entity is not None:\n",
    "                entity_size_list = counter.get(entity, [])\n",
    "                entity_size_list.append(size)\n",
    "                counter[entity] = entity_size_list\n",
    "\n",
    "\n",
    "def count_entity_size(data):\n",
    "    counter = {}\n",
    "    count_entity_size_dataset(data.train, counter)\n",
    "    count_entity_size_dataset(data.validation, counter)\n",
    "    count_entity_size_dataset(data.test, counter)\n",
    "    return counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from sklearn.linear_model import Perceptron, LogisticRegressionCV\n",
    "from sklearn.metrics import classification_report, precision_score, f1_score\n",
    "\n",
    "\n",
    "capitalized = \"^[A-Z].*$\"\n",
    "allcapitalized = \"^[A-Z]*$\"\n",
    "alldigits = \"^[0-9]*$\"\n",
    "alphanumeric = \"^[A-Za-z0-9]*$\"\n",
    "\n",
    "\n",
    "def get_entity(word):\n",
    "    if word is None:\n",
    "        return 'unknown'\n",
    "    else:\n",
    "        return word.entity\n",
    "\n",
    "\n",
    "def get_token(word):\n",
    "    if word is None:\n",
    "        return 'unknown'\n",
    "    else:\n",
    "        return word.token\n",
    "\n",
    "\n",
    "def get_stem(word):\n",
    "    if word is None:\n",
    "        return 'unknown'\n",
    "    else:\n",
    "        return word.stem\n",
    "\n",
    "\n",
    "def is_capitalized(token):\n",
    "    return re.search(capitalized, token) is not None\n",
    "\n",
    "\n",
    "def all_capitalized(token):\n",
    "    return re.search(allcapitalized, token) is not None\n",
    "\n",
    "\n",
    "def alpha_numeric(token):\n",
    "    return re.search(alphanumeric, token) is not None\n",
    "\n",
    "\n",
    "def all_digits(token):\n",
    "    return re.search(alldigits, token) is not None\n",
    "\n",
    "\n",
    "def make_feature_vec(word, prev, prev_prev, next, next_next):\n",
    "    vec = []\n",
    "    vec.append(get_entity(prev_prev))\n",
    "    vec.append(get_entity(prev))\n",
    "\n",
    "    vec.append(alpha_numeric(word.token))\n",
    "    vec.append(all_digits(word.token))\n",
    "    vec.append(all_capitalized(word.token))\n",
    "\n",
    "    # extract 3gram chars from and group the by entity\n",
    "\n",
    "    # vec.append(get_stem(prev_prev))\n",
    "    # vec.append(get_stem(prev))\n",
    "    # vec.append(get_stem(word))\n",
    "    # vec.append(get_stem(next))\n",
    "    # vec.append(get_stem(next_next))\n",
    "\n",
    "    vec.append(is_capitalized(get_token(prev_prev)))\n",
    "    vec.append(is_capitalized(get_token(prev)))\n",
    "    vec.append(is_capitalized(word.token))\n",
    "    vec.append(is_capitalized(get_token(next)))\n",
    "    vec.append(is_capitalized(get_token(next_next)))\n",
    "\n",
    "    return vec\n",
    "\n",
    "def fit_transform_column(encoders, input, column):\n",
    "    lbe = preprocessing.LabelEncoder()\n",
    "    encoders.append(lbe)\n",
    "    return lbe.fit_transform(input[:, column])\n",
    "\n",
    "\n",
    "def transform_column(encoders, input, column):\n",
    "    return encoders[column].transform(input[column])\n",
    "\n",
    "\n",
    "def transform_vector(encoders, vector):\n",
    "    new_vector = np.zeros(vector.shape)\n",
    "    for i in range(len(vector)):\n",
    "        new_vector[i] = transform_column(encoders, vector, i)[0]\n",
    "    return new_vector\n",
    "\n",
    "\n",
    "def transform_test_features(features, encoders):\n",
    "    new_features = np.zeros(features.shape)\n",
    "    for i in range(features.shape[1]):\n",
    "        new_features[:, i] = transform_column(encoders, features, i)\n",
    "    return np.array(new_features)\n",
    "\n",
    "\n",
    "def transform_train_features(features):\n",
    "    encoders = []\n",
    "    new_features = np.zeros(features.shape)\n",
    "    for i in range(features.shape[1]):\n",
    "        new_features[:, i] = fit_transform_column(encoders, features, i)\n",
    "    return np.array(new_features), encoders\n",
    "\n",
    "def get_prev(words, i, offset):\n",
    "    if i >= offset:\n",
    "        return words[i-offset]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_next(words, i, offset):\n",
    "    if i+offset < len(words):\n",
    "        return words[i + offset]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_features(data):\n",
    "    features = []\n",
    "    Y = []\n",
    "    for doc in data.documents:\n",
    "        for sentance in doc.sentences:\n",
    "            n = len(sentance.words)\n",
    "            for i in range(n):\n",
    "                prev = get_prev(sentance.words, i, 1)\n",
    "                prev_prev = get_prev(sentance.words, i, 2)\n",
    "                next = get_next(sentance.words, i, 1)\n",
    "                next_next = get_next(sentance.words, i, 2)\n",
    "                features.append(make_feature_vec(sentance.words[i], prev, prev_prev, next, next_next))\n",
    "                Y.append(sentance.words[i].entity)\n",
    "    return np.array(features), np.array(Y)\n",
    "\n",
    "\n",
    "\n",
    "def current_milli_time():\n",
    "    return int(round(time.time() * 1000))\n",
    "\n",
    "\n",
    "def print_ms(message, t1, t2):\n",
    "    print(message, t2-t1, 'ms')\n",
    "\n",
    "\n",
    "def print_help(parser, message):\n",
    "    parser.print_help()\n",
    "    print(message)\n",
    "    exit(1)\n",
    "\n",
    "\n",
    "def check_argument_set(arg_set, choices, parser):\n",
    "    for arg in arg_set:\n",
    "        if arg not in choices:\n",
    "            print_help(parser, \"'\"+arg+\"' is not in possible choices: \"+str(choices))\n",
    "\n",
    "\n",
    "def get_set(_set, languages):\n",
    "    dataset = Dataset()\n",
    "    for lang in languages:\n",
    "        data = get_data(lang)\n",
    "        dataset.merge(data.get_set(_set))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_serialized_sets(_set, languages):\n",
    "    dataset = Dataset()\n",
    "    for lang in languages:\n",
    "        with open('serialization/' + _set + '.' + lang, 'rb') as handle:\n",
    "            train = pickle.load(handle)\n",
    "            dataset.merge(train)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def save_data(train, name1, validation, name2, test, name3):\n",
    "    with open('../serialization/'+name1, 'wb') as handle:\n",
    "        pickle.dump(train, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open('../serialization/'+name2, 'wb') as handle:\n",
    "        pickle.dump(validation, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open('../serialization/'+name3, 'wb') as handle:\n",
    "        pickle.dump(test, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_data(name1, name2, name3):\n",
    "    with open('serialization/'+name1, 'rb') as handle:\n",
    "        train = pickle.load(handle)\n",
    "    with open('serialization/' + name2, 'rb') as handle:\n",
    "        validation = pickle.load(handle)\n",
    "    with open('serialization/'+name3, 'rb') as handle:\n",
    "        test = pickle.load(handle)\n",
    "    return train, validation, test\n",
    "\n",
    "\n",
    "def parse_arguments(args):\n",
    "    choices = ['eng', 'esp', 'ned']\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-train')\n",
    "    parser.add_argument('-validation')\n",
    "    parser.add_argument('-test')\n",
    "\n",
    "    parsed_args = parser.parse_args(args[1:])\n",
    "    if None in [parsed_args.train, parsed_args.test]:\n",
    "        print_help(parser, 'Must provide both train and test sets.')\n",
    "\n",
    "    train_sets = parsed_args.train.split(',')\n",
    "    validation_sets = parsed_args.validation.split(',')\n",
    "    test_sets = parsed_args.test.split(',')\n",
    "\n",
    "    print('checking arguments')\n",
    "    check_argument_set(train_sets, choices, parser)\n",
    "    check_argument_set(validation_sets, choices, parser)\n",
    "    check_argument_set(test_sets, choices, parser)\n",
    "    return train_sets, validation_sets, test_sets\n",
    "\n",
    "\n",
    "def get_datasets(train_sets, validation_sets, test_sets):\n",
    "    print('loading train, validation and test set')\n",
    "    t0 = current_milli_time()\n",
    "    train = get_serialized_sets('train', train_sets)\n",
    "    validation = get_serialized_sets('validation', validation_sets)\n",
    "    test = get_serialized_sets('test', test_sets)\n",
    "    # train = get_set('train', train_sets)\n",
    "    # validation = get_set('validation', validation_sets)\n",
    "    # test = get_set('test', test_sets)\n",
    "    # train, validation, test = load_data('train.'+parsed_args.train, 'validation.'+parsed_args.validation,\n",
    "    #                                     'test.'+parsed_args.test)\n",
    "    print_ms('data loaded in: ', t0, current_milli_time())\n",
    "\n",
    "    # save_data(train, 'train.'+parsed_args.train, validation, 'validation.'+parsed_args.validation,\n",
    "    #           test, 'test.'+parsed_args.test)\n",
    "    return train, validation, test\n",
    "\n",
    "\n",
    "def get_all_features(train, validation, test):\n",
    "    t1 = current_milli_time()\n",
    "    print('\\ngetting train features')\n",
    "    train_features, train_y = get_features(train)\n",
    "    t2 = current_milli_time()\n",
    "    print_ms('train features: ', t1, t2)\n",
    "\n",
    "    print('\\ngetting validation features')\n",
    "    validation_features, validation_y = get_features(validation)\n",
    "    t3 = current_milli_time()\n",
    "    print_ms('validation features: ', t2, t3)\n",
    "\n",
    "    print('\\ngetting test features')\n",
    "    test_features, test_y = get_features(test)\n",
    "    t4 = current_milli_time()\n",
    "    print_ms('test features: ', t3, t4)\n",
    "    return train_features, train_y, validation_features, validation_y, test_features, test_y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_args(train, test):\n",
    "    return ['/home/stipan/dev/fer/seminar/src/baseline.py', '-train', train, '-validation', train, '-test', test]\n",
    "\n",
    "\n",
    "def all_combinations():\n",
    "    for i in ['eng', 'esp', 'ned']:\n",
    "        for j in ['eng', 'esp', 'ned']:\n",
    "            main(make_args(i, j))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def transform_all_features(train_features, validation_features, test_features):\n",
    "    train_len, validation_len, test_len = len(train_features), len(validation_features), len(test_features)\n",
    "#     all_features = []\n",
    "#     all_features.extend(train_features)\n",
    "#     all_features.extend(validation_features)\n",
    "#     all_features.extend(test_features)\n",
    "    \n",
    "    all_features = np.append(np.append(train_features, validation_features, axis=0), test_features, axis=0)\n",
    "    new_features = np.zeros(all_features.shape)\n",
    "    for i in range(all_features.shape[1]):\n",
    "        new_features[:, i] = fit_transform_column(encoders, all_features, i)\n",
    "\n",
    "    ohe = preprocessing.OneHotEncoder()\n",
    "    new_features = ohe.fit_transform(new_features).toarray()\n",
    "    return new_features[:train_len, :], new_features[train_len:train_len+validation_len, :], \\\n",
    "           new_features[train_len+validation_len:, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking arguments\n",
      "loading train, validation and test set\n",
      "data loaded in:  1514 ms\n",
      "\n",
      "getting train features\n",
      "train features:  3780 ms\n",
      "\n",
      "getting validation features\n",
      "validation features:  809 ms\n",
      "\n",
      "getting test features\n",
      "test features:  753 ms\n"
     ]
    }
   ],
   "source": [
    "args = ['/home/stipan/dev/fer/seminar/src/baseline.py', '-train', 'eng', '-validation', 'eng', '-test', 'eng']\n",
    "train_sets, validation_sets, test_sets = parse_arguments(args)\n",
    "train, validation, test = get_datasets(train_sets, validation_sets, test_sets)\n",
    "train_features, train_y, validation_features, validation_y, test_features, test_y = get_all_features(train, validation, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "transforming features\n",
      "203621\n",
      "51362\n",
      "46435\n",
      "###################\n",
      "Features transform:  1625 ms\n",
      "(203621, 36)\n",
      "(51362, 36)\n",
      "(46435, 36)\n"
     ]
    }
   ],
   "source": [
    "print('\\ntransforming features')\n",
    "print(len(train_features))\n",
    "print(len(validation_features))\n",
    "print(len(test_features))\n",
    "print('###################')\n",
    "t5 = current_milli_time()\n",
    "train_features, validation_features, test_features = transform_all_features(train_features, validation_features, test_features)\n",
    "print_ms('Features transform: ', t5, current_milli_time())\n",
    "print(train_features.shape)\n",
    "print(validation_features.shape)\n",
    "print(test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "print('\\ntraining model')\n",
    "\n",
    "t6 = current_milli_time()\n",
    "# lr = LogisticRegressionCV(class_weight='balanced', n_jobs=-1, max_iter=300000, multi_class='multinomial')\n",
    "# lr.fit(train_features, train_y)\n",
    "best_estimator = None\n",
    "max_f1_micro = None\n",
    "max_f1_macro = None\n",
    "for alpha in [10**i for i in range(-10, -8)]:\n",
    "    p = Perceptron(n_jobs=-1, alpha=alpha, penalty='l2', shuffle=True)\n",
    "    p.fit(train_features, train_y)\n",
    "\n",
    "    print_ms('\\ntraining done: ', t6, current_milli_time())\n",
    "    predicted_y = p.predict(validation_features)\n",
    "    temp_f1 = f1_score(validation_y, predicted_y, average='micro')\n",
    "    if max_f1_micro is None or max_f1_micro < temp_f1:\n",
    "        max_f1_micro = temp_f1\n",
    "        max_f1_macro = f1_score(validation_y, predicted_y, average='macro')\n",
    "        best_estimator = p\n",
    "    print(p.get_params())\n",
    "    print(classification_report(validation_y, predicted_y))\n",
    "    print(\"micro: \", precision_score(validation_y, predicted_y, average='micro'))\n",
    "    print(\"macro: \", precision_score(validation_y, predicted_y, average='macro'))\n",
    "    print(\"#\"*100)\n",
    "# print(len(train.documents))\n",
    "# print(len(validation.documents))\n",
    "# print(len(test.documents))\n",
    "# all_features = []\n",
    "# all_features.extend(train_features)\n",
    "# all_features.extend(validation_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.append(train_features, validation_features, axis=0)\n",
    "b = np.append(train_y, validation_y)\n",
    "best_estimator.fit(a, b)\n",
    "predicted_y = best_estimator.predict(test_features)\n",
    "print(classification_report(test_y, predicted_y))\n",
    "print(\"micro: \", precision_score(test_y, predicted_y, average='micro'))\n",
    "print(\"macro: \", precision_score(test_y, predicted_y, average='macro'))\n",
    "print(\"#\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make_scorer(fbeta_score, beta=2)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
      " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
      " 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
      " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0),\n",
       "       fit_params={}, iid=True, n_jobs=1, param_grid={'C': [1, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=make_scorer(fbeta_score, beta=2), verbose=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "ftwo_scorer = make_scorer(fbeta_score, beta=2)\n",
    "print(ftwo_scorer)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_breast_cancer()\n",
    "X = iris.data[:, :2]  # we only take the first two features.\n",
    "Y = iris.target\n",
    "print(Y)\n",
    "grid = GridSearchCV(LinearSVC(), param_grid={'C': [0.00001,0.0001,0.001,0.01, 0.1,1, 10]}, scoring=ftwo_scorer)\n",
    "grid.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
